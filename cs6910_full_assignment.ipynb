{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIEvcvhjalfw"
   },
   "source": [
    "\n",
    "# Functions needed for construction of our neural network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o4pDXaK65aNO"
   },
   "outputs": [],
   "source": [
    "def weights_bias(weight_init, no_of_neuron, hid_layer, X_train, no_of_classes):\n",
    "  \n",
    "  n, L = no_of_neuron, hid_layer\n",
    "  Weights = []\n",
    "  bias    = []\n",
    "  np.random.seed(0)\n",
    "\n",
    "  if weight_init ==\"random\":\n",
    "    # initialize weights\n",
    "    #temp1 = np.random.rand(n, len(X_train[0]))\n",
    "    temp1 = np.random.uniform(-0.5, 0.5, size=(n, len(X_train[0])))\n",
    "    Weights.append(temp1)\n",
    "    \n",
    "    for i in range(1, L-1):\n",
    "      temp1 = np.random.uniform(-0.5, 0.5, size=(n, n))\n",
    "      Weights.append(temp1)\n",
    "\n",
    "    #temp1 = np.random.rand(no_of_classes, n)\n",
    "    temp1 = np.random.uniform(-0.5, 0.5, size=(no_of_classes, n))\n",
    "    Weights.append(temp1)\n",
    "    \n",
    "    # initialize bias\n",
    "    for i in range(L-1):\n",
    "      temp2 = np.random.uniform(-0.5, 0.5, n)\n",
    "      bias.append(temp2)\n",
    "    temp2 = np.random.uniform(-0.5, 0.5, no_of_classes)\n",
    "    bias.append(temp2)\n",
    "\n",
    "  if weight_init ==\"xavier\":\n",
    "    \n",
    "    # initialize weights\n",
    "    temp1 = np.random.randn(n, len(X_train[0]))/np.sqrt(len(X_train[0])) \n",
    "    Weights.append(temp1)\n",
    "    for i in range(1, L-1):\n",
    "      temp1  = np.random.randn(n,n)/np.sqrt(n)\n",
    "      Weights.append(temp1)\n",
    "\n",
    "    temp1 = np.random.randn(no_of_classes, n)/np.sqrt(no_of_classes)\n",
    "    Weights.append(temp1)\n",
    "\n",
    "    # initialize bias\n",
    "    for i in range(L-1):\n",
    "      temp2  = np.random.randn(n)/np.sqrt(n)   # for schochastic GD\n",
    "      bias.append(temp2)\n",
    "\n",
    "    temp2 = np.random.randn(no_of_classes)/np.sqrt(no_of_classes)   # for schochastic GD\n",
    "    bias.append(temp2)\n",
    "\n",
    "\n",
    "  if weight_init ==3:\n",
    "    # initialize weights\n",
    "    temp = np.zeros((n, len(X_train[0])))\n",
    "    Weights.append(temp)\n",
    "\n",
    "    for i in range(1, L-1):\n",
    "      temp = np.zeros((n, n))\n",
    "      Weights.append(temp)\n",
    "\n",
    "    temp = np.zeros((no_of_classes, n))\n",
    "    Weights.append(temp)\n",
    "\n",
    "    # initialize bias\n",
    "    for i in range(L-1):\n",
    "      temp = np.zeros(n)\n",
    "      bias.append(temp)\n",
    "\n",
    "    temp = np.zeros(no_of_classes)\n",
    "    bias.append(temp)\n",
    "\n",
    "  \n",
    "  return Weights, bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNe2k9o5fJCZ"
   },
   "source": [
    "Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zQeYcLR3eZXr"
   },
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "  #a = np.clip(a, -1, 1)  # clipping the value od a\n",
    "  return 1/(1+np.exp(-a))\n",
    "\n",
    "def tanh(a):\n",
    "  return (np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
    "\n",
    "def ReLu(a):  # leaky Relu\n",
    "  return np.maximum(0 ,a)\n",
    "  \n",
    "def softmax(a):\n",
    "  a = a - np.max(a)\n",
    "  return np.exp(a)/np.sum(np.exp(a))\n",
    "   \n",
    "\n",
    "def der_softmax(a):\n",
    "  return softmax(a)*(1- softmax(a))\n",
    "  \n",
    "def der_sigmoid(a):\n",
    "  return sigmoid(a)*(1-sigmoid(a))\n",
    "\n",
    "def der_tanh(a):\n",
    "  return 1-(tanh(a)*tanh(a))\n",
    "\n",
    "def der_ReLu(a):\n",
    "\n",
    "  # it will create a matrix of same dimension as of a.\n",
    "  gradient = np.zeros_like(a)  \n",
    "  # sets the entries of gradient to 1 where the corresponding entries of x>=0\n",
    "  gradient[a >=0] = 1\n",
    "  gradient[a < 0] = 0\n",
    "\n",
    "  return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVMnOZ5LI-Ps"
   },
   "source": [
    "Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7D-NSGVpI9jt"
   },
   "outputs": [],
   "source": [
    "# def cross_entropy_loss(y_dash, y_train, X_train):\n",
    "#   losses = -np.log(y_dash[y_train])\n",
    "#   return losses\n",
    "\n",
    "# def MSE_loss(y_dash, y_train, X_train):\n",
    "#   y_train_modified = np.zeros(10)\n",
    "#   y_train_modified[y_train] = 1\n",
    "#   losses = (np.sum((y_dash - y_train_modified)**2))\n",
    "#   return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfJidXdSY0F4"
   },
   "source": [
    "# Question 1\n",
    "Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset. Show each sample class in wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RntaGW8UZaXC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m3AuAaMUZbUw",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hiclips-ask\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hiclips-ask\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hiclips-ask\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hiclips-ask\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hiclips-ask\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\hiclips-ask\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (0.13.11)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from wandb) (3.10.0.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from wandb) (3.20.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from wandb) (58.0.4)\n",
      "Requirement already satisfied: pathtools in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.4)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hiclips-ask\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RVWesfE9ZgLw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n",
      "11501568/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "X_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2] )/255\n",
    "X_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "utJgTTC3ZuY0"
   },
   "outputs": [],
   "source": [
    "validation_size = int(len(X_train)*0.1)\n",
    "\n",
    "# randomly shuffle the indices of the data\n",
    "shuffled_indices = np.random.permutation(len(X_train))\n",
    "\n",
    "# split the shuffled data into training and validation sets\n",
    "train_indices, validation_indices = shuffled_indices[:-validation_size], shuffled_indices[-validation_size:]\n",
    "X_train, X_validation = X_train[train_indices], X_train[validation_indices]\n",
    "y_train, y_validation = y_train[train_indices], y_train[validation_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBS5D-MMaIOD",
    "outputId": "8277b15c-2aee-466e-853e-50f4c0fa1247"
   },
   "outputs": [],
   "source": [
    "hid_layer = int(input(\"Enter the number of Hidden + outer layer: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PamXMq9AaKVq",
    "outputId": "72c55548-0e46-430d-bff4-8a2e31292bd4"
   },
   "outputs": [],
   "source": [
    "no_of_neuron = int(input(\"Enter the numbers of neuron in each hidden layer: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0iWHQYlaQaD"
   },
   "outputs": [],
   "source": [
    "no_of_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rhbT_MXaVO3",
    "outputId": "02edecf0-797b-423a-a04f-5cef9dc41dac"
   },
   "outputs": [],
   "source": [
    "weight_init = input(\"For random weights initialisation enter random and for xavier enter xavier: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "x0HaV8ppZGKD",
    "outputId": "29b9efe6-da56-45f0-f78b-7e66bb364adb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init(entity= \"am22s020\", project=\"cs6910_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cylXBOBnY9Jd"
   },
   "outputs": [],
   "source": [
    "def plot_class_sample():\n",
    "  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "                'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "            \n",
    "  no_of_classes = len(class_names)\n",
    "\n",
    "  list_of_images  = []   # to give to the wandb\n",
    "\n",
    "  for i in range(no_of_classes):\n",
    "    \n",
    "      # Find the index of the first image of each class\n",
    "      idx = np.where(y_train == i)[0][0]\n",
    "      \n",
    "      # Plot the image\n",
    "      image = X_train[idx].reshape(28,28)\n",
    "      list_of_images.append((image, class_names[i]))\n",
    "\n",
    "  # Plot the images in a grid\n",
    "  fig, axes = plt.subplots(1, no_of_classes, figsize=(12,5))\n",
    "  for i in range(no_of_classes):\n",
    "      image, label = list_of_images[i]\n",
    "      axes[i].imshow(image, cmap='gray')\n",
    "      axes[i].set_title(label)\n",
    "      axes[i].axis('off')\n",
    "    \n",
    "  plt.show()\n",
    "\n",
    "  wandb.log({\"Question 1\": [wandb.Image(img, caption=caption) for img, caption in list_of_images]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_class_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRsgs5EJypw4"
   },
   "source": [
    "# Question 2 \n",
    "\n",
    "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NBojkEJe_I4s"
   },
   "outputs": [],
   "source": [
    "def batch_normalize(a):\n",
    "    mean = np.mean(a, axis=0, keepdims=True)\n",
    "    var = np.var(a, axis=0, keepdims=True)\n",
    "    a_norm = (a - mean) / np.sqrt(var + 1e-5)\n",
    "    return a_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jOypoQjEt81D"
   },
   "outputs": [],
   "source": [
    "def forward_propagation( Weights, bias, x_input, hid_layer, acti_fun, weight_init):\n",
    "  \n",
    "  \n",
    "  L = hid_layer\n",
    "  h = x_input\n",
    "  a_out = []\n",
    "  h_out = []\n",
    "  h_out.append(h)\n",
    "  \n",
    "  \n",
    "  ## for hidden layers\n",
    "  for k in range(L-1):\n",
    "    \n",
    "    a = np.matmul(Weights[k], h) + bias[k]\n",
    "    a_out.append(a)\n",
    "    #h = batch_normalize(h)\n",
    "    ## default activation function is sigmoid \n",
    "    if acti_fun == 'sigmoid':\n",
    "      h = sigmoid(a)\n",
    "    elif acti_fun == 'ReLu':\n",
    "      a = batch_normalize(a)\n",
    "      h = ReLu(a)\n",
    "    elif acti_fun == 'tanh':\n",
    "      h = tanh(a)\n",
    "    h_out.append(h)\n",
    "\n",
    "  ## In outer layer softmax function\n",
    "  a = np.matmul(Weights[L-1], h) + bias[L-1]\n",
    "  a_out.append(a)\n",
    "  #a = batch_normalize(a)\n",
    "  y_dash = softmax(a)\n",
    "  \n",
    "\n",
    "  return a_out, h_out, y_dash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNSQ35cHX1WO"
   },
   "source": [
    "# Question-3\n",
    "Implement the backpropagation algorithm with support for the following optimisation functions\n",
    "\n",
    "    sgd\n",
    "    momentum based gradient descent\n",
    "    nesterov accelerated gradient descent\n",
    "    rmsprop\n",
    "    adam\n",
    "    nadam \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2DZ8Ewi05Nn"
   },
   "source": [
    "Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqNajG06a2AF"
   },
   "outputs": [],
   "source": [
    "Weights, bias =  weights_bias(weight_init, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Z6s4dnd6ykc4"
   },
   "outputs": [],
   "source": [
    "def backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,\n",
    "                         hid_layer, loss_fu, acti_fun, L2_decay):\n",
    "  \n",
    "  L = hid_layer\n",
    "  grad_W = [0]*L\n",
    "  grad_b = [0]*L\n",
    "\n",
    "  \n",
    "  ## change each y_train into an array of 10 values\n",
    "  y_train_modified = np.zeros(10)\n",
    "  y_train_modified[y_train] = 1\n",
    "  \n",
    "  L2_loss = 0\n",
    "  \n",
    "  for i in range(len(Weights)):\n",
    "      L2_loss += L2_decay*np.sum(Weights[i])/len(X_train)\n",
    "  if loss_fu =='cross_entropy':\n",
    "    output_gradient = -(y_train_modified - y_dash) + L2_loss\n",
    "  elif loss_fu == 'mse':\n",
    "    output_gradient = (y_dash-y_train_modified )*der_softmax(a_out[L-1]) + L2_loss\n",
    "    \n",
    "  for k in range(L, 0, -1):\n",
    "\n",
    "    ## compute gradients w.r.t parameters\n",
    "    W_gradient = np.matmul(output_gradient.reshape(len(output_gradient),1), \n",
    "                           h_out[k-1].reshape(1,len(h_out[k-1]))) \n",
    "    grad_W[k-1] = W_gradient\n",
    "\n",
    "    b_gradients = output_gradient \n",
    "    grad_b[k-1] = b_gradients\n",
    "   \n",
    "    if k==1:\n",
    "      continue\n",
    "    ## compute gradients w.r.t layer below\n",
    "    weight = Weights[k-1]\n",
    "    h_gradient = np.matmul(weight.T, output_gradient)\n",
    "\n",
    "    ## compute the gradient of pre activation layer\n",
    "    if acti_fun == 'sigmoid':\n",
    "      output_gradient = np.multiply(h_gradient, der_sigmoid(a_out[k-2]))\n",
    "    elif acti_fun == 'ReLu':\n",
    "      output_gradient = np.multiply(h_gradient, der_ReLu(a_out[k-2]))\n",
    "    elif acti_fun == 'tanh':\n",
    "     output_gradient = np.multiply(h_gradient, der_tanh(a_out[k-2]))\n",
    "    \n",
    "\n",
    "  return grad_W, grad_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIoiBYBjdF-b"
   },
   "source": [
    "Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Y9-hsiQddEvS"
   },
   "outputs": [],
   "source": [
    "def model_loss(X, Y, Weights, bias, hid_layer, loss_fu,\n",
    "               L2_decay, X_train, acti_fun,weight_init):\n",
    "  \n",
    "  L = hid_layer\n",
    "  loss = 0\n",
    "  for x,y in zip(X, Y):\n",
    "      _, _, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "      \n",
    "      if loss_fu == 'cross_entropy':\n",
    "#         if y_dash[y]==0:\n",
    "#             continue\n",
    "        loss+= -np.log2(y_dash[y])\n",
    "      elif loss_fu == 'mse':\n",
    "        y_train_modified = np.zeros(10)\n",
    "        y_train_modified[y] = 1\n",
    "        loss1 = (np.sum((y_dash - y_train_modified)**2))\n",
    "   \n",
    "#   # Adding L2 regularization loss after an epochS\n",
    "  loss2 = 0\n",
    "  for i in range(len(Weights)):\n",
    "    loss2+= L2_decay*np.sum(Weights[i]**2)\n",
    "  \n",
    "  loss = (loss+loss2)/len(X)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HICLIP~1\\AppData\\Local\\Temp/ipykernel_7000/2596161308.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.log(1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Jf_j_EtFJhd"
   },
   "source": [
    "Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4vQxChQuUQDP"
   },
   "outputs": [],
   "source": [
    "def model_accuracy(X, Y, Weights, bias, hid_layer, acti_fun,weight_init):\n",
    "  \n",
    "  L = hid_layer\n",
    "  y_pred = np.zeros((len(X), 10))\n",
    "  i=0\n",
    "  for x in X:\n",
    "    _, _, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "    y_pred[i] = y_dash\n",
    "    i+=1\n",
    "\n",
    "  correct = 0\n",
    "  for array,y in zip(y_pred, Y):\n",
    "    if np.argmax(array)==y:\n",
    "      correct+=1\n",
    "  accuracy = correct*100/len(X)\n",
    "\n",
    "  return  accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsS21O64Kvzl"
   },
   "source": [
    "# Mini Batch Gradient Descent \n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent.\n",
    "if batch_size = Number of samples, the algorithm will be vanilla gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lF615oFSKuv-"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(learning_rate, Weights, bias, hid_layer, no_of_neuron,\n",
    "                     y_train, X_train, batch_size, L2_decay, loss_fu, acti_fun,\n",
    "                     weight_init):\n",
    "  \n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L = hid_layer\n",
    "  n = no_of_neuron\n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "  ## initialize the count of images paased\n",
    "  loss =0\n",
    "  num_points_seen = 0\n",
    "\n",
    "  for x,y in zip(X_train,y_train):\n",
    "\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "\n",
    "    ## Adding the gradients of weights and biases\n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "    \n",
    "    num_points_seen+=1\n",
    "    \n",
    "    if num_points_seen%batch_size == 0:\n",
    "        \n",
    "      #if acti_fun == 'ReLu':\n",
    "      #dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "\n",
    "      # Weights updates\n",
    "      \n",
    "      Weights = [Weights[i] - dW[i]*learning_rate for i in range(L)]\n",
    "    \n",
    "#       # normalize the weights if activation function is ReLu\n",
    "#       if acti_fun == 'ReLu':\n",
    "#          Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - dB[i]*learning_rate for i in range(L)]\n",
    "\n",
    "      \n",
    "      ## initialize the gradients of weights and biases\n",
    "      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "  # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "               L2_decay, X_train, acti_fun,weight_init)\n",
    " \n",
    "\n",
    "  #print(epoch, loss)\n",
    "\n",
    "  return Weights, bias, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O11C1AEYPgFI",
    "outputId": "923e7e43-f99a-47d5-a1bd-6c73a53f2ad8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "  Weights, bias, loss = gradient_descent(0.0001, Weights, bias, hid_layer, no_of_neuron,\n",
    "                     y_train, X_train, 10, 0.0005, 'cross_entropy', 'ReLu',\n",
    "                     weight_init)\n",
    "  print(i, loss)\n",
    "  \n",
    "# Finish the WandB run\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M68_c6qgw8dS",
    "outputId": "8d79b6fe-d7eb-4061-a29e-b28306c5b891"
   },
   "outputs": [],
   "source": [
    "val_accuracy  = model_accuracy(X_train, y_train, Weights, bias, hid_layer, 'tanh','random')\n",
    "test_accuracy = model_accuracy(X_test, y_test, Weights, bias, hid_layer, 'tanh','random')\n",
    "val_loss      = model_loss(X_train, y_train, Weights, bias, hid_layer, 'cross_entropy',\n",
    "               0.5, X_train, 'sigmoid','random')\n",
    "test_loss     =  model_loss(X_test, y_test, Weights, bias, hid_layer, 'cross_entropy',\n",
    "               0.5, X_train, 'sigmoid','random')\n",
    "print('val_accuracy',val_accuracy,'test_accuracy',test_accuracy,'val_loss', val_loss,'test_loss',test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK6JEr8ORuE4"
   },
   "source": [
    "# Mini Batch Momentum based Gradient Descent\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_u9VRqbBuly"
   },
   "outputs": [],
   "source": [
    "prev_uw, prev_ub = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "N9hSwUCaRVVz"
   },
   "outputs": [],
   "source": [
    "def momentum_gd(prev_uw, prev_ub, Weights, bias, hid_layer,\n",
    "                no_of_neuron, X_train, y_train,learning_rate,\n",
    "                batch_size, L2_decay, loss_fu, acti_fun, weight_init):\n",
    "  \n",
    "  beta = 0.9\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L = hid_layer\n",
    "  n = no_of_neuron\n",
    "  \n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "  \n",
    "  # initialize the sample count\n",
    "  num_points_seen = 0\n",
    "\n",
    "  for x,y in zip(X_train, y_train):\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "\n",
    "    ## Adding the gradients of weights and biases\n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "\n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "      \n",
    "     # normalizing the gradient\n",
    "      dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "\n",
    "      ## momentum based wight updates\n",
    "      uw = [prev_uw[i]*beta + dW[i] for i in range(len(dW))]\n",
    "      ub = [prev_ub[i]*beta + dB[i] for i in range(len(dB))]\n",
    "      \n",
    "      ## Weights and biases updates\n",
    "      # Weights updates\n",
    "      Weights = [Weights[i] - uw[i]*learning_rate for i in range(len(uw))]\n",
    "    \n",
    "#       # normalize the weights if activation function is ReLu\n",
    "#       if acti_fun == 'ReLu':\n",
    "      #Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - ub[i]*learning_rate for i in range(len(ub))]\n",
    "\n",
    "      # assign present to the history \n",
    "      prev_uw = uw\n",
    "      prev_ub = ub\n",
    "\n",
    "      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "  \n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "  \n",
    "\n",
    "  return  Weights, bias, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a-rcveBQD_w"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  Weights, bias, loss = momentum_gd(prev_uw, prev_ub, Weights, bias, hid_layer,\n",
    "                no_of_neuron, X_train, y_train, 0.001,\n",
    "                16, 0.0005, 'cross_entropy', 'ReLu', weight_init)\n",
    "  print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0-_k-mbz5h6",
    "outputId": "5d5f4e3b-1a91-4d86-fdce-b267242f756d"
   },
   "outputs": [],
   "source": [
    "val_accuracy  = model_accuracy(X_train, y_train, Weights, bias, hid_layer, 'ReLu','xavier')\n",
    "test_accuracy = model_accuracy(X_test, y_test, Weights, bias, hid_layer, 'ReLu','xavier')\n",
    "val_loss      = model_loss(X_train, y_train, Weights, bias,hid_layer, 'cross_entropy',\n",
    "               0.5, X_train, 'ReLu','random')\n",
    "test_loss     =  model_loss(X_test, y_test, Weights, bias,hid_layer, 'cross_entropy',\n",
    "               0.5, X_train, 'ReLu','random')\n",
    "print('train_accuracy',val_accuracy,'test_accuracy',test_accuracy,'train_loss', val_loss,'test_loss',test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights, bias =  weights_bias(weight_init, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2p-6WdQ0alr"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(X, Y, no_of_classes, Weights, bias,\n",
    "                     hid_layer, acti_fun, weight_init):\n",
    "\n",
    "  # initializing the confusion matrix\n",
    "  conf_matrix = np.zeros((no_of_classes, no_of_classes))\n",
    "  count_class = np.zeros(no_of_classes)\n",
    "\n",
    "  for x, y in zip(X,Y):\n",
    "    _, _, y_dash = forward_propagation(Weights, bias, x, hid_layer, acti_fun, weight_init)\n",
    "    j = np.argmax(y_dash)  # index for predicted label\n",
    "    conf_matrix[y][j] +=1\n",
    "    count_class[y] +=1\n",
    "   \n",
    "  for i in range(no_of_classes):\n",
    "    conf_matrix[i] = conf_matrix[i]/(count_class[i] + 1)\n",
    "\n",
    "  return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6AAvdeLg6e7"
   },
   "outputs": [],
   "source": [
    "wandb.init(entity= \"am22s020\", project=\"cs6910_trial_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "id": "n1w5Uwhx4rrD",
    "outputId": "d177f4a1-9496-445c-a13e-2ae43691485c"
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "conf_matrix = confusion_matrix(X_test, y_test, no_of_classes, Weights, bias,\n",
    "                     hid_layer, 'ReLu', weight_init)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "#confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "im = ax.imshow(conf_matrix, cmap='coolwarm')  # blues, coolwarm, plasma, inferno, viridis\n",
    "\n",
    "# Add colorbar\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "# Add axis labels\n",
    "ax.set_xlabel('Predicted labels', fontweight='bold')\n",
    "ax.set_ylabel('True labels', fontweight='bold')\n",
    "\n",
    "# class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "                'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Add tick labels\n",
    "tick_marks = np.arange(len(class_names))\n",
    "ax.set_xticks(tick_marks)\n",
    "ax.set_yticks(tick_marks)\n",
    "ax.set_xticklabels(class_names,  rotation=90)\n",
    "ax.set_yticklabels(class_names)\n",
    "\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "\n",
    "# Add text annotations\n",
    "thresh = conf_matrix.max() / 2.\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(j, i, format(conf_matrix[i, j], '.2f'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "#ax.grid(True)\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# converting the plot into an image and uploading to wandb\n",
    "# buf = BytesIO()\n",
    "# fig.canvas.print_png(buf)\n",
    "# buf.seek(0)\n",
    "# image = wandb.Image(np.array(plt.imread(buf)))\n",
    "# wandb.log({\"myplot\": image})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcBoS2WQeF-n"
   },
   "source": [
    "### Nesterov Accelerated Gradient Descent - MiniBatch\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1JfKzJWBAb7"
   },
   "outputs": [],
   "source": [
    "prev_vw,prev_vb = weights_bias(3, no_of_neuron, hid_layer, X_train, 10)\n",
    "#prev_vb = biases(3, 32, 3, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "EhjA6UuceE_v"
   },
   "outputs": [],
   "source": [
    "def nag(prev_vw, prev_vb, Weights, bias, hid_layer,\n",
    "        no_of_neuron, X_train, y_train,learning_rate,\n",
    "        batch_size, L2_decay, loss_fu, acti_fun, weight_init):\n",
    "    \n",
    "  \n",
    "  beta = 0.9\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L = hid_layer\n",
    "  n = no_of_neuron\n",
    "  \n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "  \n",
    "  num_points_seen = 0\n",
    "\n",
    "  # do partial updates\n",
    "  v_w = [beta*prev_vw[i] for i in range(len(prev_vw))]\n",
    "  v_b = [beta*prev_vb[i] for i in range(len(prev_vb))]\n",
    "\n",
    "  for x, y in zip(X_train, y_train):\n",
    "\n",
    "\n",
    "    ## Forward propagation\n",
    "   \n",
    "    Weights = [Weights[i]-v_w[i] for i in range(len(Weights))]\n",
    "    bias    = [bias[i]-v_b[i] for i in range(len(bias))]\n",
    "    \n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "    \n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights, \n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "\n",
    "    ## Look Ahead\n",
    "    ## Adding the gradients of weights and biases\n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "    \n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "        \n",
    "       # normalize the gradient if activation function is ReLu\n",
    "      #if acti_fun == 'ReLu':\n",
    "      #dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "\n",
    "\n",
    "      ## momentum based wight updates\n",
    "      vw = [prev_vw[i]*beta + dW[i] for i in range(len(dW))]\n",
    "      vb = [prev_vb[i]*beta + dB[i] for i in range(len(dB))]\n",
    "\n",
    "      ## Weights and biases updates\n",
    "      Weights = [Weights[i] - vw[i]*learning_rate for i in range(len(vw))]\n",
    "        \n",
    "#       # normalize the weights if activation function is ReLu\n",
    "#       if acti_fun == 'ReLu':\n",
    "      #Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - vb[i]*learning_rate for i in range(len(vb))]\n",
    "\n",
    "      # assign present to the history \n",
    "      prev_uw = vw\n",
    "      prev_ub = vb\n",
    "\n",
    "      #dW,dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "      \n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "\n",
    "\n",
    "  return  Weights, bias, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFL7QpxmVKoZ",
    "outputId": "6126801d-6a3d-461e-a897-a895b421f2e8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "  Weights, bias, loss = nag(prev_vw, prev_vb, Weights, bias, hid_layer,\n",
    "        no_of_neuron, X_train, y_train, 0.1,\n",
    "        16, 0.5, 'cross_entropy', 'ReLu', weight_init)\n",
    "  print(i, loss) \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ti8ADwESXAGY"
   },
   "source": [
    "### Adaptive Gradient(AdaGrad) based Gradient Descent- Minibatch\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PVkK0IgAoeh"
   },
   "outputs": [],
   "source": [
    "v_w, v_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ktWLOxF2WnnD"
   },
   "outputs": [],
   "source": [
    "def adagrad(v_w, v_b, Weights, bias, hid_layer, no_of_neuron, X_train, y_train,\n",
    "            learning_rate, batch_size, L2_decay, loss_fu, acti_fun,weight_init):\n",
    "  \n",
    "  v_w, v_b = weights_bias(3, no_of_neuron, hid_layer, X_train, 10)\n",
    "  \n",
    "  eps = 1e-10\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L = hid_layer\n",
    "  n = no_of_neuron\n",
    "  \n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW,dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "   \n",
    "  ## initialize the count \n",
    "  num_points_seen = 0\n",
    "\n",
    "  for x, y in zip(X_train, y_train):\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "\n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "\n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "\n",
    "      ## Add L2 regularization penalty to gradient\n",
    "      dW = [dW[i] + L2_decay*Weights[i] for i in range(len(dW))]\n",
    "\n",
    "      #compute intermediate values\n",
    "      v_w = [v_w[i] + dW[i]**2 for i in range(len(grad_W))]\n",
    "      v_b = [v_b[i] + dB[i]**2 for i in range(len(grad_b))]\n",
    "\n",
    "      # Weights updates\n",
    "      Weights = [Weights[i] - learning_rate*dW[i]/(np.sqrt(v_w[i])+eps) for i in range(len(Weights))]\n",
    "    \n",
    "      # normalize the weights if activation function is ReLu\n",
    "      #if acti_fun == 'ReLu':\n",
    "      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - learning_rate*dB[i]/(np.sqrt(v_b[i])+eps) for i in range(len(bias))]\n",
    "\n",
    "      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "      #dB = biases(3, n, L, y_train, no_of_classes)\n",
    "  \n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "\n",
    "\n",
    "  return  Weights, bias, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmF6kRdHX6-v"
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    Weights, bias, loss = adagrad(v_w, v_b,Weights, bias, hid_layer, no_of_neuron, X_train, y_train,\n",
    "            0.001, 50, 0.5, 'cross_entropy', 'sigmoid',weight_init)\n",
    "    print(i, loss)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qwy4m-glvWKq"
   },
   "source": [
    "### Root Mean Squared Propagation(RMSProp) Gradient Descent - MiniBatch\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQd_wf2TAWpJ"
   },
   "outputs": [],
   "source": [
    "v_w, v_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-HH3ZkpwvcJS"
   },
   "outputs": [],
   "source": [
    "def rmsprop(v_w, v_b, Weights, bias, hid_layer, no_of_neuron,\n",
    "            X_train, y_train, learning_rate, batch_size,\n",
    "            L2_decay,loss_fu, acti_fun, weight_init):\n",
    "  \n",
    "  eps = 1e-10\n",
    "  beta = 0.9\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L = hid_layer\n",
    "  n = no_of_neuron\n",
    "  \n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "  \n",
    "  ## initialize the sample count\n",
    "  num_points_seen = 0\n",
    "\n",
    "  for x, y in zip(X_train, y_train):\n",
    "\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "    \n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "\n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "        \n",
    "      dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "\n",
    "      ## Add L2 regularization penalty to gradient\n",
    "      #dW = [dW[i] + L2_decay*Weights[i] for i in range(len(dW))]\n",
    "\n",
    "      #compute intermediate values\n",
    "      v_w = [beta*v_w[i] + (1-beta)*(dW[i]**2) for i in range(len(grad_W))]\n",
    "      v_b = [beta*v_b[i] + (1-beta)*(dB[i]**2) for i in range(len(grad_b))]\n",
    "\n",
    "      ## Weights and biases updates\n",
    "      # Weights updates\n",
    "      Weights = [Weights[i] - learning_rate*dW[i]/(np.sqrt(v_w[i])+eps) for i in range(len(Weights))]\n",
    "        \n",
    "      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    " \n",
    "      # Biases updates\n",
    "      bias = [bias[i] - learning_rate*dB[i]/(np.sqrt(v_b[i])+eps) for i in range(len(bias))]\n",
    "\n",
    "      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "  \n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "\n",
    "  return  Weights, bias, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NjTeTrU1Nbv",
    "outputId": "b9042df1-f7e0-4f35-a0cc-f1f8a5714c1d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    Weights, bias, loss = rmsprop(v_w, v_b, Weights, bias, hid_layer, no_of_neuron,\n",
    "            X_train, y_train, 0.0001, 8,\n",
    "            0.0005, 'cross_entropy', 'sigmoid', weight_init)\n",
    "    print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy  = model_accuracy(X_train, y_train, Weights, bias, hid_layer, 'ReLu','random')\n",
    "test_accuracy = model_accuracy(X_test, y_test, Weights, bias, hid_layer, 'ReLu','random')\n",
    "val_loss      = model_loss(X_train, y_train, Weights, bias,hid_layer, 'cross_entropy',\n",
    "               0.0005, X_train, 'tanh','random')\n",
    "test_loss     =  model_loss(X_test, y_test, Weights, bias,hid_layer, 'cross_entropy',\n",
    "               0.0005, X_train, 'tanh','random')\n",
    "print('train_accuracy',val_accuracy,'test_accuracy',test_accuracy,'train_loss', val_loss,'test_loss',test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CLGHkDl4t5b"
   },
   "source": [
    "### Adaptive Delta(AdaDelta) gradient descent - Minibatch\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tERzRzZr_q5d"
   },
   "outputs": [],
   "source": [
    "u_w, u_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "v_w, v_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FAnqIPVXZj9q"
   },
   "outputs": [],
   "source": [
    "def adaDelta(u_w, u_b, v_w, v_b, Weights, bias, hid_layer,\n",
    "             no_of_neuron, X_train, y_train, batch_size,\n",
    "              L2_decay, loss_fu, acti_fun, weight_init):\n",
    "\n",
    "\n",
    "  beta = 0.9\n",
    "  eps = 1e-10\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L, n = hid_layer, no_of_neuron\n",
    "\n",
    "\n",
    "\n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "  ## initialize the sample count\n",
    "  num_points_seen = 0\n",
    "\n",
    "  for x, y in zip(X_train, y_train):\n",
    "\n",
    "    #x = np.float128(x)\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "\n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "\n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "        \n",
    "      # normalize the gradient if activation function is ReLu\n",
    "      #if acti_fun == 'ReLu':\n",
    "      dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "\n",
    "      ## Add L2 regularization penalty to gradient\n",
    "     # dW = [dW[i] + L2_decay*Weights[i] for i in range(len(dW))]\n",
    "\n",
    "      #compute intermediate values\n",
    "      v_w = [beta*v_w[i] + (1-beta)*(dW[i]**2) for i in range(len(grad_W))]\n",
    "      v_b = [beta*v_b[i] + (1-beta)*(dB[i]**2) for i in range(len(grad_b))]\n",
    "\n",
    "      del_w = [dW[i]*np.sqrt(u_w[i]+eps)/(np.sqrt(v_w[i]+eps)) for i in range(len(dW))]\n",
    "      del_b = [dB[i]*np.sqrt(u_b[i]+eps)/(np.sqrt(v_b[i]+eps)) for i in range(len(dB))]\n",
    "\n",
    "      u_w = [beta*u_w[i] + (1-beta)*del_w[i]**2 for i in range(len(u_w))]\n",
    "      u_b = [beta*u_b[i] + (1-beta)*del_b[i]**2 for i in range(len(u_b))]\n",
    "\n",
    "      # Weights updates\n",
    "      Weights = [Weights[i] - del_w[i] for i in range(len(del_w))]\n",
    "        \n",
    "#       # normalize the weights if activation function is ReLu\n",
    "#       if acti_fun == 'ReLu':\n",
    "      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - del_b[i] for i in range(len(del_b))]\n",
    "\n",
    "      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "\n",
    "\n",
    "  return  Weights, bias, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iTGhRTO5aBdv",
    "outputId": "9a40e217-1173-4ac0-a358-b910a76c4d5a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    Weights, bias, loss =adaDelta(u_w, u_b, v_w, v_b, Weights, bias, hid_layer,\n",
    "             no_of_neuron, X_train, y_train, 16,\n",
    "              0.0005, 'cross_entropy', 'tanh', weight_init)\n",
    "    print(i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BklUlyYNE2V6"
   },
   "source": [
    "### Adaptive moments(Adam) Gradient Descent- MiniBatch\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDIGDxZ4wCKZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "m_w, m_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "v_w, v_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights, bias =  weights_bias(weight_init, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "pCvLuniEE1Z_"
   },
   "outputs": [],
   "source": [
    "def adam(epoch, m_w, m_b, v_w, v_b, Weights,\n",
    "         bias, hid_layer, no_of_neuron, X_train, y_train, \n",
    "         batch_size, learning_rate, L2_decay, loss_fu, acti_fun, weight_init):\n",
    "  \n",
    "  eps = 1e-10\n",
    "  beta1 = 0.9\n",
    "  beta2 = 0.999  \n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L, n = hid_layer, no_of_neuron\n",
    "    \n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "  \n",
    "  ## initialize the sample count\n",
    "  num_points_seen = 0\n",
    "  \n",
    "  for x, y in zip(X_train, y_train):\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "    \n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "\n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "      \n",
    "     # normalize the gradient if activation function is ReLu\n",
    "#       if acti_fun == 'ReLu':\n",
    "      dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "            \n",
    "\n",
    "      #compute intermediate values\n",
    "      m_w = [beta1*m_w[i] + (1-beta1)*dW[i] for i in range(len(m_w))]\n",
    "      m_b = [beta1*m_b[i] + (1-beta1)*dB[i] for i in range(len(m_b))]\n",
    "\n",
    "      v_w = [beta2*v_w[i] + (1-beta2)*(dW[i]**2) for i in range(len(dW))]\n",
    "      v_b = [beta2*v_b[i] + (1-beta2)*(dB[i]**2) for i in range(len(dB))]\n",
    "\n",
    "      m_w_hat = [m_w[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_w))]\n",
    "      m_b_hat = [m_b[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_b))]\n",
    "\n",
    "      v_w_hat = [v_w[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_w))]\n",
    "      v_b_hat = [v_b[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_b))]\n",
    "\n",
    "      # Weights updates\n",
    "      Weights = [Weights[i] - learning_rate*m_w_hat[i]/(np.sqrt(v_w_hat[i])+eps) for i in range(len(Weights))]\n",
    "      \n",
    "#       # normalize the weights if activation function is ReLu\n",
    "#       if acti_fun == 'ReLu':\n",
    "      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - learning_rate*m_b_hat[i]/(np.sqrt(v_b_hat[i])+eps) for i in range(len(Weights))]\n",
    "\n",
    "      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "\n",
    "\n",
    "  return Weights, bias, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kaWlRUbkeTw",
    "outputId": "fb861428-8097-4789-e7bd-8614ad860b70"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(10):\n",
    "    Weights, bias, loss = adam(epoch, m_w, m_b, v_w, v_b, Weights,\n",
    "         bias, hid_layer, no_of_neuron, X_train, y_train, \n",
    "         16, 0.0001, 0.5, 'cross_entropy', 'ReLu', weight_init)\n",
    "    print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RD_lhshLj0Jk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGmz0pRpBPaG"
   },
   "source": [
    "### NAG + Adam = NAdam Gradient descent - MiniBatch\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHLwISFxvPfi"
   },
   "outputs": [],
   "source": [
    "m_w ,m_b= weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "v_w, v_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "GLRgSWWOGSeZ"
   },
   "outputs": [],
   "source": [
    "def nadam(epoch, m_w, m_b, v_w, v_b, Weights, bias,\n",
    "          hid_layer, no_of_neuron, X_train, y_train, batch_size,\n",
    "          learning_rate,L2_decay, loss_fu, acti_fun, weight_init):\n",
    "  \n",
    "  eps = 1e-10\n",
    "  beta1 = 0.9\n",
    "  beta2 = 0.999\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L, n = hid_layer, no_of_neuron\n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3,no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "\n",
    "  ## initialize the sample count\n",
    "  num_points_seen = 0\n",
    "\n",
    "  for x, y in zip(X_train, y_train):\n",
    "\n",
    "    #x = np.float128(x)\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "    \n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "\n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "        \n",
    "      # normalize the gradient if activation function is ReLu\n",
    "      #if acti_fun == 'ReLu':\n",
    "      dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "\n",
    "      ## Add L2 regularization penalty to gradient\n",
    "      #dW = [dW[i] + L2_decay*Weights[i]/batch_size for i in range(len(dW))]\n",
    "\n",
    "      #compute intermediate values\n",
    "      m_w = [beta1*m_w[i] + (1-beta1)*dW[i] for i in range(len(m_w))]\n",
    "      m_b = [beta1*m_b[i] + (1-beta1)*dB[i] for i in range(len(m_b))]\n",
    "\n",
    "      v_w = [beta2*v_w[i] + (1-beta2)*(dW[i]**2) for i in range(len(dW))]\n",
    "      v_b = [beta2*v_b[i] + (1-beta2)*(dB[i]**2) for i in range(len(dB))]\n",
    "\n",
    "      m_w_hat = [m_w[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_w))]\n",
    "      m_b_hat = [m_b[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_b))]\n",
    "\n",
    "      v_w_hat = [v_w[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_w))]\n",
    "      v_b_hat = [v_b[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_b))]\n",
    "\n",
    "      # Weights updates\n",
    "      Weights = [Weights[i] - (learning_rate/np.sqrt(v_w_hat[i]+eps))*(beta1*m_w_hat[i]+(1-beta1)*dW[i]/(1-beta1**(epoch+1))) for i in range(len(Weights))]\n",
    "      \n",
    "      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - (learning_rate/np.sqrt(v_b_hat[i]+eps))*(beta1*m_b_hat[i]+(1-beta1)*dB[i]/(1-beta1**(epoch+1))) for i in range(len(Weights))]\n",
    "\n",
    "      dW, dB = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "\n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "\n",
    "  return Weights, bias, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYpytPVUn4PZ"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(10):\n",
    "    Weights, bias, loss = nadam(epoch, m_w, m_b, v_w, v_b, Weights, bias,\n",
    "          hid_layer, no_of_neuron, X_train, y_train, 16,\n",
    "          0.0001,0.5, 'cross_entropy', 'ReLu', weight_init)\n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh5tIV_XaOyg"
   },
   "source": [
    "# Question-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OJ_1yvk0HuGk"
   },
   "outputs": [],
   "source": [
    "def train_NN():\n",
    "  \n",
    "  # default values\n",
    "  config_defaults = {\n",
    "        'max_epochs': 10,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 1e-3,\n",
    "        'acti_fun': 'sigmoid',\n",
    "        'optimizer': 'sgd',\n",
    "        'weight_init': 'random',\n",
    "        'L2_decay': 0,\n",
    "        'no_of_neuron': 16,\n",
    "        'hid_layer': 3,\n",
    "        'loss_fu':'cross_entropy'\n",
    "    }\n",
    "  \n",
    "  # initialize wandb\n",
    "  wandb.init(config=config_defaults)\n",
    "\n",
    "  # config is a data structure that holds hyperparameters and inputs\n",
    "  config = wandb.config\n",
    "\n",
    "  # Local variables, values obtained from wandb config\n",
    "  no_of_neuron = config.no_of_neuron\n",
    "  hid_layer = config.hid_layer\n",
    "  weight_init = config.weight_init\n",
    "  max_epochs = config.max_epochs\n",
    "  batch_size = config.batch_size\n",
    "  learning_rate = config.learning_rate\n",
    "  acti_fun = config.acti_fun\n",
    "  L2_decay = config.L2_decay\n",
    "  optimizer = config.optimizer\n",
    "  loss_fu = config.loss_fu\n",
    "\n",
    "  wandb.run.name  = \"loss_{}_opt_{}_e_{}_nhl_{}_shl_{}_lr_{}_bs_{}_W_{}_af_{}_L2_{}\".format(loss_fu,\n",
    "                                                                              optimizer,\n",
    "                                                                              max_epochs,\n",
    "                                                                              hid_layer,\n",
    "                                                                              no_of_neuron,\n",
    "                                                                              learning_rate,\n",
    "                                                                              batch_size,\n",
    "                                                                              weight_init,\n",
    "                                                                              acti_fun, L2_decay)\n",
    "                                                                              \n",
    "                                                                                  \n",
    "  \n",
    "  print(wandb.run.name )\n",
    "\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  Weights, bias = weights_bias(weight_init, no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "  eps = 1e-10\n",
    "  beta1 = 0.9\n",
    "  beta2 = 0.999\n",
    "  L, n = hid_layer, no_of_neuron\n",
    "  \n",
    "  prev_uw, prev_ub = weights_bias(3, n, L, X_train, no_of_classes)\n",
    " \n",
    "  prev_vw, prev_vb = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "  \n",
    "  m_w, m_b = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "  v_w, v_b  = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "  u_w, u_b  = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "  for epoch in range(max_epochs):\n",
    "\n",
    "    if optimizer == 'sgd':\n",
    "      Weights, bias, loss = gradient_descent(learning_rate, Weights, bias, hid_layer, no_of_neuron,\n",
    "                                           y_train, X_train, batch_size, L2_decay, loss_fu, acti_fun,\n",
    "                                           weight_init)\n",
    "    elif optimizer == 'momentum':\n",
    "      Weights, bias, loss = momentum_gd(prev_uw, prev_ub, Weights, bias, hid_layer,\n",
    "                                       no_of_neuron, X_train, y_train,learning_rate,\n",
    "                                        batch_size, L2_decay, loss_fu, acti_fun, weight_init)\n",
    "    elif optimizer == 'adaDelta':\n",
    "      Weights, bias, loss = adaDelta(u_w, u_b, v_w, v_b, Weights, bias, hid_layer,\n",
    "                                     no_of_neuron, X_train, y_train, batch_size,\n",
    "                                     L2_decay, loss_fu, acti_fun, weight_init)\n",
    "    elif optimizer == 'rmsprop':\n",
    "      Weights, bias, loss = rmsprop(v_w, v_b, Weights, bias, hid_layer, no_of_neuron,\n",
    "                                    X_train, y_train, learning_rate, batch_size,\n",
    "                                    L2_decay,loss_fu, acti_fun, weight_init)\n",
    "    elif optimizer == 'adam':\n",
    "      Weights, bias, loss = adam(epoch, m_w, m_b, v_w, v_b, Weights,\n",
    "                                 bias, hid_layer, no_of_neuron, X_train, y_train, \n",
    "                                 batch_size, learning_rate, L2_decay, loss_fu, acti_fun, weight_init)\n",
    "    elif optimizer == 'nadam':   \n",
    "      Weights, bias, loss = nadam(epoch, m_w, m_b, v_w, v_b, Weights, bias,\n",
    "                                  hid_layer, no_of_neuron, X_train, y_train, batch_size,\n",
    "                                  learning_rate,L2_decay, loss_fu, acti_fun, weight_init)\n",
    "\n",
    "    print(epoch, loss)\n",
    "\n",
    "  \n",
    "    val_accuracy  = model_accuracy(X_validation, y_validation, Weights, bias, hid_layer, acti_fun,weight_init)\n",
    "    train_accuracy = model_accuracy(X_train, y_train, Weights, bias, hid_layer, acti_fun,weight_init)\n",
    "    val_loss      = model_loss(X_validation, y_validation, Weights, bias, hid_layer, loss_fu,\n",
    "                               L2_decay, X_train, acti_fun, weight_init)\n",
    "    train_loss     = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                                L2_decay, X_train, acti_fun,weight_init)\n",
    "    \n",
    "  \n",
    "    wandb.log({\"validation accuracy\": val_accuracy, \"train accuracy\": train_accuracy, \"validation loss\": val_loss, \"train loss\": train_loss, 'epoch': epoch})\n",
    "    \n",
    "  wandb.run.name \n",
    "  wandb.run.save()\n",
    "  wandb.run.finish()\n",
    "\n",
    "  return Weights, bias, loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NoPTtMACUUF"
   },
   "source": [
    "\n",
    "#W&B Sweep\n",
    "\n",
    "In this cell, we set up the configurations for the various hyperparameters and use the Sweeps feature to find the combination that gives us the highest validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 764
    },
    "id": "4XSwvYkDYuzZ",
    "outputId": "1aeeeaa8-33d7-4f6a-b5ce-63c6dc7029a7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: kanaatih\n",
      "Sweep URL: https://wandb.ai/am22s020/cs6910_final/sweeps/kanaatih\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6ty3mjoh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tL2_decay: 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tacti_fun: ReLu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layer: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_of_neuron: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: xavier\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\HICLIPS-ASK\\wandb\\run-20230319_151711-6ty3mjoh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/am22s020/cs6910_final/runs/6ty3mjoh' target=\"_blank\">cerulean-sweep-1</a></strong> to <a href='https://wandb.ai/am22s020/cs6910_final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/am22s020/cs6910_final/sweeps/kanaatih' target=\"_blank\">https://wandb.ai/am22s020/cs6910_final/sweeps/kanaatih</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/am22s020/cs6910_final' target=\"_blank\">https://wandb.ai/am22s020/cs6910_final</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/am22s020/cs6910_final/sweeps/kanaatih' target=\"_blank\">https://wandb.ai/am22s020/cs6910_final/sweeps/kanaatih</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/am22s020/cs6910_final/runs/6ty3mjoh' target=\"_blank\">https://wandb.ai/am22s020/cs6910_final/runs/6ty3mjoh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_cross_entropy_opt_adam_e_10_nhl_3_shl_128_lr_0.001_bs_8_W_xavier_af_ReLu_L2_0.0005\n",
      "0 3.6678225742475346\n",
      "1 3.3625773476284686\n",
      "2 3.331541181731593\n",
      "3 3.3264174954980024\n",
      "4 3.3241496654121288\n",
      "5 3.323214416222384\n",
      "6 3.322660522542675\n",
      "7 3.3225250636174266\n",
      "8 3.322600731122704\n",
      "9 3.322768564443892\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-40:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 2090, in _atexit_cleanup\n",
      "    self._on_finish()\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 2323, in _on_finish\n",
      "    _ = exit_handle.wait(timeout=-1, on_progress=self._on_progress_exit)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 298, in wait\n",
      "    on_probe(probe_handle)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 2288, in _on_probe_exit\n",
      "    result = handle.wait(timeout=0)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 281, in wait\n",
      "    raise MailboxError(\"transport failed\")\n",
      "wandb.sdk.lib.mailbox.MailboxError: transport failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 298, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\HICLIP~1\\AppData\\Local\\Temp/ipykernel_11992/1239060120.py\", line 108, in train_NN\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 368, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 331, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1843, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1858, in _finish\n",
      "    self._atexit_cleanup(exit_code=exit_code)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 2101, in _atexit_cleanup\n",
      "    self._backend.cleanup()\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\backend\\backend.py\", line 259, in cleanup\n",
      "    self.interface.join()\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 638, in join\n",
      "    super().join()\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 741, in join\n",
      "    _ = self._communicate_shutdown()\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 551, in _communicate_shutdown\n",
      "    _ = self._communicate(record)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 285, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 60, in _communicate_async\n",
      "    future = self._router.send_and_receive(rec, local=local)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\router.py\", line 94, in send_and_receive\n",
      "    self._send_message(rec)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\interface\\router_sock.py\", line 36, in _send_message\n",
      "    self._sock_client.send_record_communicate(record)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 216, in send_record_communicate\n",
      "    self.send_server_request(server_req)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 303, in _run_job\n",
      "    wandb.finish(exit_code=1)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 3669, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 368, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 331, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1843, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1856, in _finish\n",
      "    hook.call()\n",
      "  File \"C:\\Users\\HICLIPS-ASK\\anaconda3\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 442, in _jupyter_teardown\n",
      "    ipython.display_pub.publish = ipython.display_pub._orig_publish\n",
      "AttributeError: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\"name\": \"cs6910_final\", \"method\": \"grid\"}   \n",
    "sweep_config[\"metric\"] = {\"name\": \"val_accuracy\", \"goal\": \"maximize\"}\n",
    "\n",
    "parameters_dict = {\n",
    "              \"max_epochs\": {\"values\": [10]},\n",
    "              \"hid_layer\": {\"values\": [3]},  \n",
    "              \"no_of_neuron\": {\"values\": [128]},           \n",
    "              \"learning_rate\": {\"values\": [1e-3]},\n",
    "              \"optimizer\": {\"values\": [\"adam\"]},\n",
    "              \"batch_size\": {\"values\": [8]}, \n",
    "              \"weight_init\": {\"values\": [\"xavier\"]} ,\n",
    "              \"L2_decay\": {\"values\": [0.0005]} ,\n",
    "              \"acti_fun\": {\"values\": [\"ReLu\"]}, \n",
    "                }\n",
    "sweep_config[\"parameters\"] = parameters_dict\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, entity=\"am22s020\", project=\"cs6910_final\")\n",
    "wandb.agent(sweep_id, train_NN, count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIEvcvhjalfw"
   },
   "source": [
    "\n",
    "# Functions needed for construction of our neural network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4pDXaK65aNO"
   },
   "outputs": [],
   "source": [
    "def weights_bias(weight_init, no_of_neuron, hid_layer, X_train, no_of_classes):\n",
    "  \n",
    "  n, L = no_of_neuron, hid_layer\n",
    "  Weights = []\n",
    "  bias    = []\n",
    "  np.random.seed(0)\n",
    "\n",
    "  if weight_init ==\"random\":\n",
    "    # initialize weights\n",
    "    #temp1 = np.random.rand(n, len(X_train[0]))\n",
    "    temp1 = np.random.uniform(-0.5, 0.5, size=(n, len(X_train[0])))\n",
    "    Weights.append(temp1)\n",
    "    \n",
    "    for i in range(1, L-1):\n",
    "      temp1 = np.random.uniform(-0.5, 0.5, size=(n, n))\n",
    "      Weights.append(temp1)\n",
    "\n",
    "    #temp1 = np.random.rand(no_of_classes, n)\n",
    "    temp1 = np.random.uniform(-0.5, 0.5, size=(no_of_classes, n))\n",
    "    Weights.append(temp1)\n",
    "    \n",
    "    # initialize bias\n",
    "    for i in range(L-1):\n",
    "      temp2 = np.random.uniform(-0.5, 0.5, n)\n",
    "      bias.append(temp2)\n",
    "    temp2 = np.random.uniform(-0.5, 0.5, no_of_classes)\n",
    "    bias.append(temp2)\n",
    "\n",
    "  if weight_init ==\"xavier\":\n",
    "    \n",
    "    # initialize weights\n",
    "    temp1 = np.random.randn(n, len(X_train[0]))/np.sqrt(len(X_train[0])) \n",
    "    Weights.append(temp1)\n",
    "    for i in range(1, L-1):\n",
    "      temp1  = np.random.randn(n,n)/np.sqrt(n)\n",
    "      Weights.append(temp1)\n",
    "\n",
    "    temp1 = np.random.randn(no_of_classes, n)/np.sqrt(no_of_classes)\n",
    "    Weights.append(temp1)\n",
    "\n",
    "    # initialize bias\n",
    "    for i in range(L-1):\n",
    "      temp2  = np.random.randn(n)/np.sqrt(n)   # for schochastic GD\n",
    "      bias.append(temp2)\n",
    "\n",
    "    temp2 = np.random.randn(no_of_classes)/np.sqrt(no_of_classes)   # for schochastic GD\n",
    "    bias.append(temp2)\n",
    "\n",
    "\n",
    "  if weight_init ==3:\n",
    "    # initialize weights\n",
    "    temp = np.zeros((n, len(X_train[0])))\n",
    "    Weights.append(temp)\n",
    "\n",
    "    for i in range(1, L-1):\n",
    "      temp = np.zeros((n, n))\n",
    "      Weights.append(temp)\n",
    "\n",
    "    temp = np.zeros((no_of_classes, n))\n",
    "    Weights.append(temp)\n",
    "\n",
    "    # initialize bias\n",
    "    for i in range(L-1):\n",
    "      temp = np.zeros(n)\n",
    "      bias.append(temp)\n",
    "\n",
    "    temp = np.zeros(no_of_classes)\n",
    "    bias.append(temp)\n",
    "\n",
    "  \n",
    "  return Weights, bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNe2k9o5fJCZ"
   },
   "source": [
    "Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQeYcLR3eZXr"
   },
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "  #a = np.clip(a, -1, 1)  # clipping the value od a\n",
    "  return 1/(1+np.exp(-a))\n",
    "\n",
    "def tanh(a):\n",
    "  return (np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
    "\n",
    "def ReLu(a):  # leaky Relu\n",
    "  return np.maximum(0 ,a)\n",
    "  \n",
    "def softmax(a):\n",
    "  a = a - np.max(a)\n",
    "  return np.exp(a)/np.sum(np.exp(a))\n",
    "   \n",
    "\n",
    "def der_softmax(a):\n",
    "  return softmax(a)*(1- softmax(a))\n",
    "  \n",
    "def der_sigmoid(a):\n",
    "  return sigmoid(a)*(1-sigmoid(a))\n",
    "\n",
    "def der_tanh(a):\n",
    "  return 1-(tanh(a)*tanh(a))\n",
    "\n",
    "def der_ReLu(a):\n",
    "\n",
    "  # it will create a matrix of same dimension as of a.\n",
    "  gradient = np.zeros_like(a)  \n",
    "  # sets the entries of gradient to 1 where the corresponding entries of x>=0\n",
    "  gradient[a >=0] = 1\n",
    "  gradient[a < 0] = 0\n",
    "\n",
    "  return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVMnOZ5LI-Ps"
   },
   "source": [
    "Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7D-NSGVpI9jt"
   },
   "outputs": [],
   "source": [
    "# def cross_entropy_loss(y_dash, y_train, X_train):\n",
    "#   losses = -np.log(y_dash[y_train])\n",
    "#   return losses\n",
    "\n",
    "# def MSE_loss(y_dash, y_train, X_train):\n",
    "#   y_train_modified = np.zeros(10)\n",
    "#   y_train_modified[y_train] = 1\n",
    "#   losses = (np.sum((y_dash - y_train_modified)**2))\n",
    "#   return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfJidXdSY0F4"
   },
   "source": [
    "# Question 1\n",
    "Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset. Show each sample class in wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RntaGW8UZaXC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3AuAaMUZbUw",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVWesfE9ZgLw"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "X_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2] )/255\n",
    "X_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utJgTTC3ZuY0"
   },
   "outputs": [],
   "source": [
    "validation_size = int(len(X_train)*0.1)\n",
    "\n",
    "# randomly shuffle the indices of the data\n",
    "shuffled_indices = np.random.permutation(len(X_train))\n",
    "\n",
    "# split the shuffled data into training and validation sets\n",
    "train_indices, validation_indices = shuffled_indices[:-validation_size], shuffled_indices[-validation_size:]\n",
    "X_train, X_validation = X_train[train_indices], X_train[validation_indices]\n",
    "y_train, y_validation = y_train[train_indices], y_train[validation_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBS5D-MMaIOD",
    "outputId": "8277b15c-2aee-466e-853e-50f4c0fa1247"
   },
   "outputs": [],
   "source": [
    "hid_layer = int(input(\"Enter the number of Hidden + outer layer: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PamXMq9AaKVq",
    "outputId": "72c55548-0e46-430d-bff4-8a2e31292bd4"
   },
   "outputs": [],
   "source": [
    "no_of_neuron = int(input(\"Enter the numbers of neuron in each hidden layer: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0iWHQYlaQaD"
   },
   "outputs": [],
   "source": [
    "no_of_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rhbT_MXaVO3",
    "outputId": "02edecf0-797b-423a-a04f-5cef9dc41dac"
   },
   "outputs": [],
   "source": [
    "weight_init = input(\"For random weights initialisation enter random and for xavier enter xavier: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "x0HaV8ppZGKD",
    "outputId": "29b9efe6-da56-45f0-f78b-7e66bb364adb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init(entity= \"am22s020\", project=\"cs6910_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cylXBOBnY9Jd"
   },
   "outputs": [],
   "source": [
    "def plot_class_sample():\n",
    "  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "                'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "            \n",
    "  no_of_classes = len(class_names)\n",
    "\n",
    "  list_of_images  = []   # to give to the wandb\n",
    "\n",
    "  for i in range(no_of_classes):\n",
    "    \n",
    "      # Find the index of the first image of each class\n",
    "      idx = np.where(y_train == i)[0][0]\n",
    "      \n",
    "      # Plot the image\n",
    "      image = X_train[idx].reshape(28,28)\n",
    "      list_of_images.append((image, class_names[i]))\n",
    "\n",
    "  # Plot the images in a grid\n",
    "  fig, axes = plt.subplots(1, no_of_classes, figsize=(12,5))\n",
    "  for i in range(no_of_classes):\n",
    "      image, label = list_of_images[i]\n",
    "      axes[i].imshow(image, cmap='gray')\n",
    "      axes[i].set_title(label)\n",
    "      axes[i].axis('off')\n",
    "    \n",
    "  plt.show()\n",
    "\n",
    "  wandb.log({\"Question 1\": [wandb.Image(img, caption=caption) for img, caption in list_of_images]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_class_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRsgs5EJypw4"
   },
   "source": [
    "# Question 2 \n",
    "\n",
    "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBojkEJe_I4s"
   },
   "outputs": [],
   "source": [
    "def batch_normalize(a):\n",
    "    mean = np.mean(a, axis=0, keepdims=True)\n",
    "    var = np.var(a, axis=0, keepdims=True)\n",
    "    a_norm = (a - mean) / np.sqrt(var + 1e-5)\n",
    "    return a_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOypoQjEt81D"
   },
   "outputs": [],
   "source": [
    "def forward_propagation( Weights, bias, x_input, hid_layer, acti_fun, weight_init):\n",
    "  \n",
    "  \n",
    "  L = hid_layer\n",
    "  h = x_input\n",
    "  a_out = []\n",
    "  h_out = []\n",
    "  h_out.append(h)\n",
    "  \n",
    "  \n",
    "  ## for hidden layers\n",
    "  for k in range(L-1):\n",
    "    \n",
    "    a = np.matmul(Weights[k], h) + bias[k]\n",
    "    a_out.append(a)\n",
    "    #h = batch_normalize(h)\n",
    "    ## default activation function is sigmoid \n",
    "    if acti_fun == 'sigmoid':\n",
    "      h = sigmoid(a)\n",
    "    elif acti_fun == 'ReLu':\n",
    "      a = batch_normalize(a)\n",
    "      h = ReLu(a)\n",
    "    elif acti_fun == 'tanh':\n",
    "      h = tanh(a)\n",
    "    h_out.append(h)\n",
    "\n",
    "  ## In outer layer softmax function\n",
    "  a = np.matmul(Weights[L-1], h) + bias[L-1]\n",
    "  a_out.append(a)\n",
    "  #a = batch_normalize(a)\n",
    "  y_dash = softmax(a)\n",
    "  \n",
    "\n",
    "  return a_out, h_out, y_dash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNSQ35cHX1WO"
   },
   "source": [
    "# Question-3\n",
    "Implement the backpropagation algorithm with support for the following optimisation functions\n",
    "\n",
    "    sgd\n",
    "    momentum based gradient descent\n",
    "    nesterov accelerated gradient descent\n",
    "    rmsprop\n",
    "    adam\n",
    "    nadam \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2DZ8Ewi05Nn"
   },
   "source": [
    "Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqNajG06a2AF"
   },
   "outputs": [],
   "source": [
    "Weights, bias =  weights_bias(weight_init, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6s4dnd6ykc4"
   },
   "outputs": [],
   "source": [
    "def backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,\n",
    "                         hid_layer, loss_fu, acti_fun, L2_decay):\n",
    "  \n",
    "  L = hid_layer\n",
    "  grad_W = [0]*L\n",
    "  grad_b = [0]*L\n",
    "\n",
    "  \n",
    "  ## change each y_train into an array of 10 values\n",
    "  y_train_modified = np.zeros(10)\n",
    "  y_train_modified[y_train] = 1\n",
    "  \n",
    "  L2_loss = 0\n",
    "  \n",
    "  for i in range(len(Weights)):\n",
    "      L2_loss += L2_decay*np.sum(Weights[i])/len(X_train)\n",
    "  if loss_fu =='cross_entropy':\n",
    "    output_gradient = -(y_train_modified - y_dash) + L2_loss\n",
    "  elif loss_fu == 'mse':\n",
    "    output_gradient = (y_dash-y_train_modified )*der_softmax(a_out[L-1]) + L2_loss\n",
    "    \n",
    "  for k in range(L, 0, -1):\n",
    "\n",
    "    ## compute gradients w.r.t parameters\n",
    "    W_gradient = np.matmul(output_gradient.reshape(len(output_gradient),1), \n",
    "                           h_out[k-1].reshape(1,len(h_out[k-1]))) \n",
    "    grad_W[k-1] = W_gradient\n",
    "\n",
    "    b_gradients = output_gradient \n",
    "    grad_b[k-1] = b_gradients\n",
    "   \n",
    "    if k==1:\n",
    "      continue\n",
    "    ## compute gradients w.r.t layer below\n",
    "    weight = Weights[k-1]\n",
    "    h_gradient = np.matmul(weight.T, output_gradient)\n",
    "\n",
    "    ## compute the gradient of pre activation layer\n",
    "    if acti_fun == 'sigmoid':\n",
    "      output_gradient = np.multiply(h_gradient, der_sigmoid(a_out[k-2]))\n",
    "    elif acti_fun == 'ReLu':\n",
    "      output_gradient = np.multiply(h_gradient, der_ReLu(a_out[k-2]))\n",
    "    elif acti_fun == 'tanh':\n",
    "     output_gradient = np.multiply(h_gradient, der_tanh(a_out[k-2]))\n",
    "    \n",
    "\n",
    "  return grad_W, grad_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIoiBYBjdF-b"
   },
   "source": [
    "Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9-hsiQddEvS"
   },
   "outputs": [],
   "source": [
    "def model_loss(X, Y, Weights, bias, hid_layer, loss_fu,\n",
    "               L2_decay, X_train, acti_fun,weight_init):\n",
    "  \n",
    "  L = hid_layer\n",
    "  loss = 0\n",
    "  for x,y in zip(X, Y):\n",
    "      _, _, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "      \n",
    "      if loss_fu == 'cross_entropy':\n",
    "#         if y_dash[y]==0:\n",
    "#             continue\n",
    "        loss+= -np.log2(y_dash[y])\n",
    "      elif loss_fu == 'mse':\n",
    "        y_train_modified = np.zeros(10)\n",
    "        y_train_modified[y] = 1\n",
    "        loss1 = (np.sum((y_dash - y_train_modified)**2))\n",
    "   \n",
    "#   # Adding L2 regularization loss after an epochS\n",
    "  loss2 = 0\n",
    "  for i in range(len(Weights)):\n",
    "    loss2+= L2_decay*np.sum(Weights[i]**2)\n",
    "  \n",
    "  loss = (loss+loss2)/len(X)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Jf_j_EtFJhd"
   },
   "source": [
    "Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vQxChQuUQDP"
   },
   "outputs": [],
   "source": [
    "def model_accuracy(X, Y, Weights, bias, hid_layer, acti_fun,weight_init):\n",
    "  \n",
    "  L = hid_layer\n",
    "  y_pred = np.zeros((len(X), 10))\n",
    "  i=0\n",
    "  for x in X:\n",
    "    _, _, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "    y_pred[i] = y_dash\n",
    "    i+=1\n",
    "\n",
    "  correct = 0\n",
    "  for array,y in zip(y_pred, Y):\n",
    "    if np.argmax(array)==y:\n",
    "      correct+=1\n",
    "  accuracy = correct*100/len(X)\n",
    "\n",
    "  return  accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsS21O64Kvzl"
   },
   "source": [
    "# Mini Batch Gradient Descent \n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent.\n",
    "if batch_size = Number of samples, the algorithm will be vanilla gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lF615oFSKuv-"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(learning_rate, Weights, bias, hid_layer, no_of_neuron,\n",
    "                     y_train, X_train, batch_size, L2_decay, loss_fu, acti_fun,\n",
    "                     weight_init):\n",
    "  \n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L = hid_layer\n",
    "  n = no_of_neuron\n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "  ## initialize the count of images paased\n",
    "  loss =0\n",
    "  num_points_seen = 0\n",
    "\n",
    "  for x,y in zip(X_train,y_train):\n",
    "\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "\n",
    "    ## Adding the gradients of weights and biases\n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "    \n",
    "    num_points_seen+=1\n",
    "    \n",
    "    if num_points_seen%batch_size == 0:\n",
    "        \n",
    "      #if acti_fun == 'ReLu':\n",
    "      #dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "\n",
    "      # Weights updates\n",
    "      \n",
    "      Weights = [Weights[i] - dW[i]*learning_rate for i in range(L)]\n",
    "    \n",
    "#       # normalize the weights if activation function is ReLu\n",
    "#       if acti_fun == 'ReLu':\n",
    "#          Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - dB[i]*learning_rate for i in range(L)]\n",
    "\n",
    "      \n",
    "      ## initialize the gradients of weights and biases\n",
    "      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "  # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "               L2_decay, X_train, acti_fun,weight_init)\n",
    " \n",
    "\n",
    "  #print(epoch, loss)\n",
    "\n",
    "  return Weights, bias, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O11C1AEYPgFI",
    "outputId": "923e7e43-f99a-47d5-a1bd-6c73a53f2ad8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "  Weights, bias, loss = gradient_descent(0.0001, Weights, bias, hid_layer, no_of_neuron,\n",
    "                     y_train, X_train, 10, 0.0005, 'cross_entropy', 'ReLu',\n",
    "                     weight_init)\n",
    "  print(i, loss)\n",
    "  \n",
    "# Finish the WandB run\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M68_c6qgw8dS",
    "outputId": "8d79b6fe-d7eb-4061-a29e-b28306c5b891"
   },
   "outputs": [],
   "source": [
    "val_accuracy  = model_accuracy(X_train, y_train, Weights, bias, hid_layer, 'tanh','random')\n",
    "test_accuracy = model_accuracy(X_test, y_test, Weights, bias, hid_layer, 'tanh','random')\n",
    "val_loss      = model_loss(X_train, y_train, Weights, bias, hid_layer, 'cross_entropy',\n",
    "               0.5, X_train, 'sigmoid','random')\n",
    "test_loss     =  model_loss(X_test, y_test, Weights, bias, hid_layer, 'cross_entropy',\n",
    "               0.5, X_train, 'sigmoid','random')\n",
    "print('val_accuracy',val_accuracy,'test_accuracy',test_accuracy,'val_loss', val_loss,'test_loss',test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK6JEr8ORuE4"
   },
   "source": [
    "# Mini Batch Momentum based Gradient Descent\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_u9VRqbBuly"
   },
   "outputs": [],
   "source": [
    "prev_uw, prev_ub = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9hSwUCaRVVz"
   },
   "outputs": [],
   "source": [
    "def momentum_gd(prev_uw, prev_ub, Weights, bias, hid_layer,\n",
    "                no_of_neuron, X_train, y_train,learning_rate,\n",
    "                batch_size, L2_decay, loss_fu, acti_fun, weight_init):\n",
    "  \n",
    "  beta = 0.9\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L = hid_layer\n",
    "  n = no_of_neuron\n",
    "  \n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "  \n",
    "  # initialize the sample count\n",
    "  num_points_seen = 0\n",
    "\n",
    "  for x,y in zip(X_train, y_train):\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "\n",
    "    ## Adding the gradients of weights and biases\n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "\n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "      \n",
    "     # normalizing the gradient\n",
    "      dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "\n",
    "      ## momentum based wight updates\n",
    "      uw = [prev_uw[i]*beta + dW[i] for i in range(len(dW))]\n",
    "      ub = [prev_ub[i]*beta + dB[i] for i in range(len(dB))]\n",
    "      \n",
    "      ## Weights and biases updates\n",
    "      # Weights updates\n",
    "      Weights = [Weights[i] - uw[i]*learning_rate for i in range(len(uw))]\n",
    "    \n",
    "#       # normalize the weights if activation function is ReLu\n",
    "#       if acti_fun == 'ReLu':\n",
    "      #Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - ub[i]*learning_rate for i in range(len(ub))]\n",
    "\n",
    "      # assign present to the history \n",
    "      prev_uw = uw\n",
    "      prev_ub = ub\n",
    "\n",
    "      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "  \n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "  \n",
    "\n",
    "  return  Weights, bias, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a-rcveBQD_w"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  Weights, bias, loss = momentum_gd(prev_uw, prev_ub, Weights, bias, hid_layer,\n",
    "                no_of_neuron, X_train, y_train, 0.001,\n",
    "                16, 0.0005, 'cross_entropy', 'ReLu', weight_init)\n",
    "  print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0-_k-mbz5h6",
    "outputId": "5d5f4e3b-1a91-4d86-fdce-b267242f756d"
   },
   "outputs": [],
   "source": [
    "val_accuracy  = model_accuracy(X_train, y_train, Weights, bias, hid_layer, 'ReLu','xavier')\n",
    "test_accuracy = model_accuracy(X_test, y_test, Weights, bias, hid_layer, 'ReLu','xavier')\n",
    "val_loss      = model_loss(X_train, y_train, Weights, bias,hid_layer, 'cross_entropy',\n",
    "               0.5, X_train, 'ReLu','random')\n",
    "test_loss     =  model_loss(X_test, y_test, Weights, bias,hid_layer, 'cross_entropy',\n",
    "               0.5, X_train, 'ReLu','random')\n",
    "print('train_accuracy',val_accuracy,'test_accuracy',test_accuracy,'train_loss', val_loss,'test_loss',test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights, bias =  weights_bias(weight_init, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2p-6WdQ0alr"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(X, Y, no_of_classes, Weights, bias,\n",
    "                     hid_layer, acti_fun, weight_init):\n",
    "\n",
    "  # initializing the confusion matrix\n",
    "  conf_matrix = np.zeros((no_of_classes, no_of_classes))\n",
    "  count_class = np.zeros(no_of_classes)\n",
    "\n",
    "  for x, y in zip(X,Y):\n",
    "    _, _, y_dash = forward_propagation(Weights, bias, x, hid_layer, acti_fun, weight_init)\n",
    "    j = np.argmax(y_dash)  # index for predicted label\n",
    "    conf_matrix[y][j] +=1\n",
    "    count_class[y] +=1\n",
    "   \n",
    "  for i in range(no_of_classes):\n",
    "    conf_matrix[i] = conf_matrix[i]/(count_class[i] + 1)\n",
    "\n",
    "  return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6AAvdeLg6e7"
   },
   "outputs": [],
   "source": [
    "wandb.init(entity= \"am22s020\", project=\"cs6910_trial_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "id": "n1w5Uwhx4rrD",
    "outputId": "d177f4a1-9496-445c-a13e-2ae43691485c"
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "conf_matrix = confusion_matrix(X_test, y_test, no_of_classes, Weights, bias,\n",
    "                     hid_layer, 'ReLu', weight_init)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "#confusion_matrix = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "im = ax.imshow(conf_matrix, cmap='coolwarm')  # blues, coolwarm, plasma, inferno, viridis\n",
    "\n",
    "# Add colorbar\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "# Add axis labels\n",
    "ax.set_xlabel('Predicted labels', fontweight='bold')\n",
    "ax.set_ylabel('True labels', fontweight='bold')\n",
    "\n",
    "# class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "                'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Add tick labels\n",
    "tick_marks = np.arange(len(class_names))\n",
    "ax.set_xticks(tick_marks)\n",
    "ax.set_yticks(tick_marks)\n",
    "ax.set_xticklabels(class_names,  rotation=90)\n",
    "ax.set_yticklabels(class_names)\n",
    "\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "\n",
    "# Add text annotations\n",
    "thresh = conf_matrix.max() / 2.\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(j, i, format(conf_matrix[i, j], '.2f'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "#ax.grid(True)\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# converting the plot into an image and uploading to wandb\n",
    "# buf = BytesIO()\n",
    "# fig.canvas.print_png(buf)\n",
    "# buf.seek(0)\n",
    "# image = wandb.Image(np.array(plt.imread(buf)))\n",
    "# wandb.log({\"myplot\": image})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcBoS2WQeF-n"
   },
   "source": [
    "### Nesterov Accelerated Gradient Descent - MiniBatch\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1JfKzJWBAb7"
   },
   "outputs": [],
   "source": [
    "prev_vw,prev_vb = weights_bias(3, no_of_neuron, hid_layer, X_train, 10)\n",
    "#prev_vb = biases(3, 32, 3, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhjA6UuceE_v"
   },
   "outputs": [],
   "source": [
    "def nag(prev_vw, prev_vb, Weights, bias, hid_layer,\n",
    "        no_of_neuron, X_train, y_train,learning_rate,\n",
    "        batch_size, L2_decay, loss_fu, acti_fun, weight_init):\n",
    "    \n",
    "  \n",
    "  beta = 0.9\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L = hid_layer\n",
    "  n = no_of_neuron\n",
    "  \n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "  \n",
    "  num_points_seen = 0\n",
    "\n",
    "  # do partial updates\n",
    "  v_w = [beta*prev_vw[i] for i in range(len(prev_vw))]\n",
    "  v_b = [beta*prev_vb[i] for i in range(len(prev_vb))]\n",
    "\n",
    "  for x, y in zip(X_train, y_train):\n",
    "\n",
    "\n",
    "    ## Forward propagation\n",
    "   \n",
    "    Weights = [Weights[i]-v_w[i] for i in range(len(Weights))]\n",
    "    bias    = [bias[i]-v_b[i] for i in range(len(bias))]\n",
    "    \n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "    \n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights, \n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "\n",
    "    ## Look Ahead\n",
    "    ## Adding the gradients of weights and biases\n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "    \n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "        \n",
    "       # normalize the gradient if activation function is ReLu\n",
    "      #if acti_fun == 'ReLu':\n",
    "      #dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "\n",
    "\n",
    "      ## momentum based wight updates\n",
    "      vw = [prev_vw[i]*beta + dW[i] for i in range(len(dW))]\n",
    "      vb = [prev_vb[i]*beta + dB[i] for i in range(len(dB))]\n",
    "\n",
    "      ## Weights and biases updates\n",
    "      Weights = [Weights[i] - vw[i]*learning_rate for i in range(len(vw))]\n",
    "        \n",
    "#       # normalize the weights if activation function is ReLu\n",
    "#       if acti_fun == 'ReLu':\n",
    "      #Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - vb[i]*learning_rate for i in range(len(vb))]\n",
    "\n",
    "      # assign present to the history \n",
    "      prev_uw = vw\n",
    "      prev_ub = vb\n",
    "\n",
    "      #dW,dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "      \n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "\n",
    "\n",
    "  return  Weights, bias, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFL7QpxmVKoZ",
    "outputId": "6126801d-6a3d-461e-a897-a895b421f2e8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "  Weights, bias, loss = nag(prev_vw, prev_vb, Weights, bias, hid_layer,\n",
    "        no_of_neuron, X_train, y_train, 0.1,\n",
    "        16, 0.5, 'cross_entropy', 'ReLu', weight_init)\n",
    "  print(i, loss) \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ti8ADwESXAGY"
   },
   "source": [
    "### Adaptive Gradient(AdaGrad) based Gradient Descent- Minibatch\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PVkK0IgAoeh"
   },
   "outputs": [],
   "source": [
    "v_w, v_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktWLOxF2WnnD"
   },
   "outputs": [],
   "source": [
    "def adagrad(v_w, v_b, Weights, bias, hid_layer, no_of_neuron, X_train, y_train,\n",
    "            learning_rate, batch_size, L2_decay, loss_fu, acti_fun,weight_init):\n",
    "  \n",
    "  v_w, v_b = weights_bias(3, no_of_neuron, hid_layer, X_train, 10)\n",
    "  \n",
    "  eps = 1e-10\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L = hid_layer\n",
    "  n = no_of_neuron\n",
    "  \n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW,dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "   \n",
    "  ## initialize the count \n",
    "  num_points_seen = 0\n",
    "\n",
    "  for x, y in zip(X_train, y_train):\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "\n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "\n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "\n",
    "      ## Add L2 regularization penalty to gradient\n",
    "      dW = [dW[i] + L2_decay*Weights[i] for i in range(len(dW))]\n",
    "\n",
    "      #compute intermediate values\n",
    "      v_w = [v_w[i] + dW[i]**2 for i in range(len(grad_W))]\n",
    "      v_b = [v_b[i] + dB[i]**2 for i in range(len(grad_b))]\n",
    "\n",
    "      # Weights updates\n",
    "      Weights = [Weights[i] - learning_rate*dW[i]/(np.sqrt(v_w[i])+eps) for i in range(len(Weights))]\n",
    "    \n",
    "      # normalize the weights if activation function is ReLu\n",
    "      #if acti_fun == 'ReLu':\n",
    "      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - learning_rate*dB[i]/(np.sqrt(v_b[i])+eps) for i in range(len(bias))]\n",
    "\n",
    "      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "      #dB = biases(3, n, L, y_train, no_of_classes)\n",
    "  \n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "\n",
    "\n",
    "  return  Weights, bias, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmF6kRdHX6-v"
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    Weights, bias, loss = adagrad(v_w, v_b,Weights, bias, hid_layer, no_of_neuron, X_train, y_train,\n",
    "            0.001, 50, 0.5, 'cross_entropy', 'sigmoid',weight_init)\n",
    "    print(i, loss)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qwy4m-glvWKq"
   },
   "source": [
    "### Root Mean Squared Propagation(RMSProp) Gradient Descent - MiniBatch\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQd_wf2TAWpJ"
   },
   "outputs": [],
   "source": [
    "v_w, v_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HH3ZkpwvcJS"
   },
   "outputs": [],
   "source": [
    "def rmsprop(v_w, v_b, Weights, bias, hid_layer, no_of_neuron,\n",
    "            X_train, y_train, learning_rate, batch_size,\n",
    "            L2_decay,loss_fu, acti_fun, weight_init):\n",
    "  \n",
    "  eps = 1e-10\n",
    "  beta = 0.9\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L = hid_layer\n",
    "  n = no_of_neuron\n",
    "  \n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "  \n",
    "  ## initialize the sample count\n",
    "  num_points_seen = 0\n",
    "\n",
    "  for x, y in zip(X_train, y_train):\n",
    "\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "    \n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "\n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "        \n",
    "      dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "\n",
    "      ## Add L2 regularization penalty to gradient\n",
    "      #dW = [dW[i] + L2_decay*Weights[i] for i in range(len(dW))]\n",
    "\n",
    "      #compute intermediate values\n",
    "      v_w = [beta*v_w[i] + (1-beta)*(dW[i]**2) for i in range(len(grad_W))]\n",
    "      v_b = [beta*v_b[i] + (1-beta)*(dB[i]**2) for i in range(len(grad_b))]\n",
    "\n",
    "      ## Weights and biases updates\n",
    "      # Weights updates\n",
    "      Weights = [Weights[i] - learning_rate*dW[i]/(np.sqrt(v_w[i])+eps) for i in range(len(Weights))]\n",
    "        \n",
    "      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    " \n",
    "      # Biases updates\n",
    "      bias = [bias[i] - learning_rate*dB[i]/(np.sqrt(v_b[i])+eps) for i in range(len(bias))]\n",
    "\n",
    "      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "  \n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "\n",
    "  return  Weights, bias, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NjTeTrU1Nbv",
    "outputId": "b9042df1-f7e0-4f35-a0cc-f1f8a5714c1d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    Weights, bias, loss = rmsprop(v_w, v_b, Weights, bias, hid_layer, no_of_neuron,\n",
    "            X_train, y_train, 0.0001, 8,\n",
    "            0.0005, 'cross_entropy', 'sigmoid', weight_init)\n",
    "    print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracy  = model_accuracy(X_train, y_train, Weights, bias, hid_layer, 'ReLu','random')\n",
    "test_accuracy = model_accuracy(X_test, y_test, Weights, bias, hid_layer, 'ReLu','random')\n",
    "val_loss      = model_loss(X_train, y_train, Weights, bias,hid_layer, 'cross_entropy',\n",
    "               0.0005, X_train, 'tanh','random')\n",
    "test_loss     =  model_loss(X_test, y_test, Weights, bias,hid_layer, 'cross_entropy',\n",
    "               0.0005, X_train, 'tanh','random')\n",
    "print('train_accuracy',val_accuracy,'test_accuracy',test_accuracy,'train_loss', val_loss,'test_loss',test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CLGHkDl4t5b"
   },
   "source": [
    "### Adaptive Delta(AdaDelta) gradient descent - Minibatch\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tERzRzZr_q5d"
   },
   "outputs": [],
   "source": [
    "u_w, u_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "v_w, v_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAnqIPVXZj9q"
   },
   "outputs": [],
   "source": [
    "def adaDelta(u_w, u_b, v_w, v_b, Weights, bias, hid_layer,\n",
    "             no_of_neuron, X_train, y_train, batch_size,\n",
    "              L2_decay, loss_fu, acti_fun, weight_init):\n",
    "\n",
    "\n",
    "  beta = 0.9\n",
    "  eps = 1e-10\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L, n = hid_layer, no_of_neuron\n",
    "\n",
    "\n",
    "\n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "  ## initialize the sample count\n",
    "  num_points_seen = 0\n",
    "\n",
    "  for x, y in zip(X_train, y_train):\n",
    "\n",
    "    #x = np.float128(x)\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "\n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "\n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "        \n",
    "      # normalize the gradient if activation function is ReLu\n",
    "      #if acti_fun == 'ReLu':\n",
    "      dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "\n",
    "      ## Add L2 regularization penalty to gradient\n",
    "     # dW = [dW[i] + L2_decay*Weights[i] for i in range(len(dW))]\n",
    "\n",
    "      #compute intermediate values\n",
    "      v_w = [beta*v_w[i] + (1-beta)*(dW[i]**2) for i in range(len(grad_W))]\n",
    "      v_b = [beta*v_b[i] + (1-beta)*(dB[i]**2) for i in range(len(grad_b))]\n",
    "\n",
    "      del_w = [dW[i]*np.sqrt(u_w[i]+eps)/(np.sqrt(v_w[i]+eps)) for i in range(len(dW))]\n",
    "      del_b = [dB[i]*np.sqrt(u_b[i]+eps)/(np.sqrt(v_b[i]+eps)) for i in range(len(dB))]\n",
    "\n",
    "      u_w = [beta*u_w[i] + (1-beta)*del_w[i]**2 for i in range(len(u_w))]\n",
    "      u_b = [beta*u_b[i] + (1-beta)*del_b[i]**2 for i in range(len(u_b))]\n",
    "\n",
    "      # Weights updates\n",
    "      Weights = [Weights[i] - del_w[i] for i in range(len(del_w))]\n",
    "        \n",
    "#       # normalize the weights if activation function is ReLu\n",
    "#       if acti_fun == 'ReLu':\n",
    "      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - del_b[i] for i in range(len(del_b))]\n",
    "\n",
    "      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "\n",
    "\n",
    "  return  Weights, bias, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iTGhRTO5aBdv",
    "outputId": "9a40e217-1173-4ac0-a358-b910a76c4d5a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    Weights, bias, loss =adaDelta(u_w, u_b, v_w, v_b, Weights, bias, hid_layer,\n",
    "             no_of_neuron, X_train, y_train, 16,\n",
    "              0.0005, 'cross_entropy', 'tanh', weight_init)\n",
    "    print(i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BklUlyYNE2V6"
   },
   "source": [
    "### Adaptive moments(Adam) Gradient Descent- MiniBatch\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDIGDxZ4wCKZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "m_w, m_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "v_w, v_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights, bias =  weights_bias(weight_init, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCvLuniEE1Z_"
   },
   "outputs": [],
   "source": [
    "def adam(epoch, m_w, m_b, v_w, v_b, Weights,\n",
    "         bias, hid_layer, no_of_neuron, X_train, y_train, \n",
    "         batch_size, learning_rate, L2_decay, loss_fu, acti_fun, weight_init):\n",
    "  \n",
    "  eps = 1e-10\n",
    "  beta1 = 0.9\n",
    "  beta2 = 0.999  \n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L, n = hid_layer, no_of_neuron\n",
    "    \n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "  \n",
    "  ## initialize the sample count\n",
    "  num_points_seen = 0\n",
    "  \n",
    "  for x, y in zip(X_train, y_train):\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "    \n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "\n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "      \n",
    "     # normalize the gradient if activation function is ReLu\n",
    "#       if acti_fun == 'ReLu':\n",
    "      dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "            \n",
    "\n",
    "      #compute intermediate values\n",
    "      m_w = [beta1*m_w[i] + (1-beta1)*dW[i] for i in range(len(m_w))]\n",
    "      m_b = [beta1*m_b[i] + (1-beta1)*dB[i] for i in range(len(m_b))]\n",
    "\n",
    "      v_w = [beta2*v_w[i] + (1-beta2)*(dW[i]**2) for i in range(len(dW))]\n",
    "      v_b = [beta2*v_b[i] + (1-beta2)*(dB[i]**2) for i in range(len(dB))]\n",
    "\n",
    "      m_w_hat = [m_w[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_w))]\n",
    "      m_b_hat = [m_b[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_b))]\n",
    "\n",
    "      v_w_hat = [v_w[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_w))]\n",
    "      v_b_hat = [v_b[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_b))]\n",
    "\n",
    "      # Weights updates\n",
    "      Weights = [Weights[i] - learning_rate*m_w_hat[i]/(np.sqrt(v_w_hat[i])+eps) for i in range(len(Weights))]\n",
    "      \n",
    "#       # normalize the weights if activation function is ReLu\n",
    "#       if acti_fun == 'ReLu':\n",
    "      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - learning_rate*m_b_hat[i]/(np.sqrt(v_b_hat[i])+eps) for i in range(len(Weights))]\n",
    "\n",
    "      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "\n",
    "\n",
    "  return Weights, bias, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kaWlRUbkeTw",
    "outputId": "fb861428-8097-4789-e7bd-8614ad860b70"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(10):\n",
    "    Weights, bias, loss = adam(epoch, m_w, m_b, v_w, v_b, Weights,\n",
    "         bias, hid_layer, no_of_neuron, X_train, y_train, \n",
    "         16, 0.0001, 0.5, 'cross_entropy', 'ReLu', weight_init)\n",
    "    print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RD_lhshLj0Jk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGmz0pRpBPaG"
   },
   "source": [
    "### NAG + Adam = NAdam Gradient descent - MiniBatch\n",
    "if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHLwISFxvPfi"
   },
   "outputs": [],
   "source": [
    "m_w ,m_b= weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "v_w, v_b = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLRgSWWOGSeZ"
   },
   "outputs": [],
   "source": [
    "def nadam(epoch, m_w, m_b, v_w, v_b, Weights, bias,\n",
    "          hid_layer, no_of_neuron, X_train, y_train, batch_size,\n",
    "          learning_rate,L2_decay, loss_fu, acti_fun, weight_init):\n",
    "  \n",
    "  eps = 1e-10\n",
    "  beta1 = 0.9\n",
    "  beta2 = 0.999\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  L, n = hid_layer, no_of_neuron\n",
    "  ## initialize the gradients of weights and biases\n",
    "  dW, dB = weights_bias(3,no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "\n",
    "  ## initialize the sample count\n",
    "  num_points_seen = 0\n",
    "\n",
    "  for x, y in zip(X_train, y_train):\n",
    "\n",
    "    #x = np.float128(x)\n",
    "\n",
    "    ## Forward propagation\n",
    "    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)\n",
    "\n",
    "    ## Backward Propagation\n",
    "    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,\n",
    "                                          hid_layer, loss_fu, acti_fun, L2_decay)\n",
    "    \n",
    "    dW = [dW[i] + grad_W[i] for i in range(len(dW))]\n",
    "    dB = [dB[i] + grad_b[i] for i in range(len(dB))]\n",
    "\n",
    "    num_points_seen +=1\n",
    "\n",
    "    if num_points_seen%batch_size==0:\n",
    "        \n",
    "      # normalize the gradient if activation function is ReLu\n",
    "      #if acti_fun == 'ReLu':\n",
    "      dW = [batch_normalize(dW[i]) for i in range(len(dW))]\n",
    "\n",
    "      ## Add L2 regularization penalty to gradient\n",
    "      #dW = [dW[i] + L2_decay*Weights[i]/batch_size for i in range(len(dW))]\n",
    "\n",
    "      #compute intermediate values\n",
    "      m_w = [beta1*m_w[i] + (1-beta1)*dW[i] for i in range(len(m_w))]\n",
    "      m_b = [beta1*m_b[i] + (1-beta1)*dB[i] for i in range(len(m_b))]\n",
    "\n",
    "      v_w = [beta2*v_w[i] + (1-beta2)*(dW[i]**2) for i in range(len(dW))]\n",
    "      v_b = [beta2*v_b[i] + (1-beta2)*(dB[i]**2) for i in range(len(dB))]\n",
    "\n",
    "      m_w_hat = [m_w[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_w))]\n",
    "      m_b_hat = [m_b[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_b))]\n",
    "\n",
    "      v_w_hat = [v_w[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_w))]\n",
    "      v_b_hat = [v_b[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_b))]\n",
    "\n",
    "      # Weights updates\n",
    "      Weights = [Weights[i] - (learning_rate/np.sqrt(v_w_hat[i]+eps))*(beta1*m_w_hat[i]+(1-beta1)*dW[i]/(1-beta1**(epoch+1))) for i in range(len(Weights))]\n",
    "      \n",
    "      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]\n",
    "\n",
    "      # Biases updates\n",
    "      bias = [bias[i] - (learning_rate/np.sqrt(v_b_hat[i]+eps))*(beta1*m_b_hat[i]+(1-beta1)*dB[i]/(1-beta1**(epoch+1))) for i in range(len(Weights))]\n",
    "\n",
    "      dW, dB = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "\n",
    "   # Training loss of an epoch\n",
    "  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                     L2_decay, X_train, acti_fun, weight_init)\n",
    "\n",
    "  return Weights, bias, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYpytPVUn4PZ"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(10):\n",
    "    Weights, bias, loss = nadam(epoch, m_w, m_b, v_w, v_b, Weights, bias,\n",
    "          hid_layer, no_of_neuron, X_train, y_train, 16,\n",
    "          0.0001,0.5, 'cross_entropy', 'ReLu', weight_init)\n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh5tIV_XaOyg"
   },
   "source": [
    "# Question-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJ_1yvk0HuGk"
   },
   "outputs": [],
   "source": [
    "def train_NN():\n",
    "  \n",
    "  # default values\n",
    "  config_defaults = {\n",
    "        'max_epochs': 10,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 1e-3,\n",
    "        'acti_fun': 'sigmoid',\n",
    "        'optimizer': 'sgd',\n",
    "        'weight_init': 'random',\n",
    "        'L2_decay': 0,\n",
    "        'no_of_neuron': 16,\n",
    "        'hid_layer': 3,\n",
    "        'loss_fu':'cross_entropy'\n",
    "    }\n",
    "  \n",
    "  # initialize wandb\n",
    "  wandb.init(config=config_defaults)\n",
    "\n",
    "  # config is a data structure that holds hyperparameters and inputs\n",
    "  config = wandb.config\n",
    "\n",
    "  # Local variables, values obtained from wandb config\n",
    "  no_of_neuron = config.no_of_neuron\n",
    "  hid_layer = config.hid_layer\n",
    "  weight_init = config.weight_init\n",
    "  max_epochs = config.max_epochs\n",
    "  batch_size = config.batch_size\n",
    "  learning_rate = config.learning_rate\n",
    "  acti_fun = config.acti_fun\n",
    "  L2_decay = config.L2_decay\n",
    "  optimizer = config.optimizer\n",
    "  loss_fu = config.loss_fu\n",
    "\n",
    "  wandb.run.name  = \"loss_{}_opt_{}_e_{}_nhl_{}_shl_{}_lr_{}_bs_{}_W_{}_af_{}_L2_{}\".format(loss_fu,\n",
    "                                                                              optimizer,\n",
    "                                                                              max_epochs,\n",
    "                                                                              hid_layer,\n",
    "                                                                              no_of_neuron,\n",
    "                                                                              learning_rate,\n",
    "                                                                              batch_size,\n",
    "                                                                              weight_init,\n",
    "                                                                              acti_fun, L2_decay)\n",
    "                                                                              \n",
    "                                                                                  \n",
    "  \n",
    "  print(wandb.run.name )\n",
    "\n",
    "  no_of_classes = len(np.unique(y_train))\n",
    "  Weights, bias = weights_bias(weight_init, no_of_neuron, hid_layer, X_train, no_of_classes)\n",
    "  eps = 1e-10\n",
    "  beta1 = 0.9\n",
    "  beta2 = 0.999\n",
    "  L, n = hid_layer, no_of_neuron\n",
    "  \n",
    "  prev_uw, prev_ub = weights_bias(3, n, L, X_train, no_of_classes)\n",
    " \n",
    "  prev_vw, prev_vb = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "  \n",
    "  m_w, m_b = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "  v_w, v_b  = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "  u_w, u_b  = weights_bias(3, n, L, X_train, no_of_classes)\n",
    "\n",
    "  for epoch in range(max_epochs):\n",
    "\n",
    "    if optimizer == 'sgd':\n",
    "      Weights, bias, loss = gradient_descent(learning_rate, Weights, bias, hid_layer, no_of_neuron,\n",
    "                                           y_train, X_train, batch_size, L2_decay, loss_fu, acti_fun,\n",
    "                                           weight_init)\n",
    "    elif optimizer == 'momentum':\n",
    "      Weights, bias, loss = momentum_gd(prev_uw, prev_ub, Weights, bias, hid_layer,\n",
    "                                       no_of_neuron, X_train, y_train,learning_rate,\n",
    "                                        batch_size, L2_decay, loss_fu, acti_fun, weight_init)\n",
    "    elif optimizer == 'adaDelta':\n",
    "      Weights, bias, loss = adaDelta(u_w, u_b, v_w, v_b, Weights, bias, hid_layer,\n",
    "                                     no_of_neuron, X_train, y_train, batch_size,\n",
    "                                     L2_decay, loss_fu, acti_fun, weight_init)\n",
    "    elif optimizer == 'rmsprop':\n",
    "      Weights, bias, loss = rmsprop(v_w, v_b, Weights, bias, hid_layer, no_of_neuron,\n",
    "                                    X_train, y_train, learning_rate, batch_size,\n",
    "                                    L2_decay,loss_fu, acti_fun, weight_init)\n",
    "    elif optimizer == 'adam':\n",
    "      Weights, bias, loss = adam(epoch, m_w, m_b, v_w, v_b, Weights,\n",
    "                                 bias, hid_layer, no_of_neuron, X_train, y_train, \n",
    "                                 batch_size, learning_rate, L2_decay, loss_fu, acti_fun, weight_init)\n",
    "    elif optimizer == 'nadam':   \n",
    "      Weights, bias, loss = nadam(epoch, m_w, m_b, v_w, v_b, Weights, bias,\n",
    "                                  hid_layer, no_of_neuron, X_train, y_train, batch_size,\n",
    "                                  learning_rate,L2_decay, loss_fu, acti_fun, weight_init)\n",
    "\n",
    "    print(epoch, loss)\n",
    "\n",
    "  \n",
    "    val_accuracy  = model_accuracy(X_validation, y_validation, Weights, bias, hid_layer, acti_fun,weight_init)\n",
    "    train_accuracy = model_accuracy(X_train, y_train, Weights, bias, hid_layer, acti_fun,weight_init)\n",
    "    val_loss      = model_loss(X_validation, y_validation, Weights, bias, hid_layer, loss_fu,\n",
    "                               L2_decay, X_train, acti_fun, weight_init)\n",
    "    train_loss     = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,\n",
    "                                L2_decay, X_train, acti_fun,weight_init)\n",
    "    \n",
    "  \n",
    "    wandb.log({\"validation accuracy\": val_accuracy, \"train accuracy\": train_accuracy, \"validation loss\": val_loss, \"train loss\": train_loss, 'epoch': epoch})\n",
    "    \n",
    "  wandb.run.name \n",
    "  wandb.run.save()\n",
    "  wandb.run.finish()\n",
    "\n",
    "  return Weights, bias, loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NoPTtMACUUF"
   },
   "source": [
    "\n",
    "#W&B Sweep\n",
    "\n",
    "In this cell, we set up the configurations for the various hyperparameters and use the Sweeps feature to find the combination that gives us the highest validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 764
    },
    "id": "4XSwvYkDYuzZ",
    "outputId": "1aeeeaa8-33d7-4f6a-b5ce-63c6dc7029a7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sweep_config = {\"name\": \"cs6910_final\", \"method\": \"grid\"}   \n",
    "sweep_config[\"metric\"] = {\"name\": \"val_accuracy\", \"goal\": \"maximize\"}\n",
    "\n",
    "parameters_dict = {\n",
    "              \"max_epochs\": {\"values\": [10, 20, 30]},\n",
    "                \"hid_layer\": {\"values\": [3, 4, 5]},  \n",
    "                \"no_of_neuron\": {\"values\": [32, 64, 128]},           \n",
    "                \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
    "                \"optimizer\": {\"values\": [\"sgd\",\"momentum\",\"nesterov\",\"rmsprop\",\"adam\",\"nadam\"]},\n",
    "                \"batch_size\": {\"values\": [16, 32, 64]}, \n",
    "                \"weight_init\": {\"values\": [\"random\", \"xavier\"]} ,\n",
    "                \"L2_decay\": {\"values\": [0, 0.0005, 0.5]} ,\n",
    "                \"acti_fun\": {\"values\": [\"sigmoid\", \"tanh\", \"ReLu\"]}, \n",
    "                }\n",
    "sweep_config[\"parameters\"] = parameters_dict\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, entity=\"am22s020\", project=\"cs6910_final\")\n",
    "wandb.agent(sweep_id, train_NN, count=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
